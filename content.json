{"meta":{"title":"Chen Shangyu's Blog","subtitle":null,"description":null,"author":"Chen Shangyu","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"Network Sparsity","slug":"Network-Sparsity","date":"2017-11-05T03:19:18.000Z","updated":"2017-11-05T05:07:38.000Z","comments":true,"path":"2017/11/05/Network-Sparsity/","link":"","permalink":"http://yoursite.com/2017/11/05/Network-Sparsity/","excerpt":"This article is about some methods to sparsify neural network by training. Recently I have read two paper published by Sung Ju Hwang group in ICML 2017. I think these two papers share some ideas. Put them here to be compared.","text":"This article is about some methods to sparsify neural network by training. Recently I have read two paper published by Sung Ju Hwang group in ICML 2017. I think these two papers share some ideas. Put them here to be compared. Combined Group and Exclusive Sparisty for Deep Neural NetworkPaper in one sentenceThis paper adds two regularizers: Group sparsity and exclusive sparsity, to train neural network. Motivation In the optimal case, the weights at each layer will be fully orthogonal to each other, and thus forming an orthogonal basis set. To do so, enforce network weights at each layer to fit to different sets of input features as much as possible. (exclusive sparsity) However, it is not practical nor desirable to restrict each weight to be completely disjoint from others as some features still need to be shared (think about low level features). Therefore introduce an additional group sparsity regularizer based on (2, 1)-norm. (group sparsity) Idea Visiualization Group Sparsity tries to delete input neurons. Exclusive Sparsity tries to make output neurons fight for input neurons. That is to say input neurons belong to only one output neuron. FormulationGroup Sparsity\\Omega (\\mathbf{W}) = \\sum_g ||\\mathbf{W}_g^l||_2 = \\sum_g \\sqrt{\\sum_i (w_{g,i}^l)^2}where: $g$ represents group, in this paper, every input neuron and its corresponding weights are a group. $l$: layer of neural network Obviously we can see, it does a group lasso in input neuron: do 2-norm at group first, then do 1-norm between groups. Exclusive Sparsity\\Omega (\\mathbf{W}) = \\frac{1}{2} \\sum_g ||\\mathbf{W}_g^l||_1^2 = \\frac{1}{2} \\sum_g (\\sum_i |w_{g,i}^l|)^2Some thinkingsIt is easy to knwo using group lasso can result in group sparsity. But it confuses me why using (1,2)-norm can “make output neurons compete for inputs”. As is said in paper: Applying 2-norm over these 1-norm groups will result in even weights among the groups; that is, all groups should have similar number of non-sparse weights, and thus no group can have large number of non-sparse weight. Also, exclusive sparsity was first introduced in Multi-task learning: Hierarchical Classification via Orthogonal Transfer Maybe can find more details in this paper. OptimizationIt uses Proximal Gradient Descent: First obtains the intermediate solution $\\mathbf{W}_{t+\\frac{1}{2}}$ by taking a gradient step using the gradient computed on the loss only. Then optimize for the regularization term while performing Euclidean projection of it to the solution space:\\min_{\\mathbf{W}_{t+1}} = \\{\\Omega (\\mathbf{W}_{t+\\frac{1}{2}}) + \\frac{1}{2\\lambda} || \\mathbf{W}_{t+1} - \\mathbf{W}_{t+\\frac{1}{2} ||_2^2} \\} Group Sparsity proximal stepf(\\mathbf{W}_{t+1}) = \\frac{1}{2} ||\\mathbf{W}_{t+1} - \\mathbf{W}_{t + \\frac{1}{2}}||^2_2 + \\lambda [\\mathbf{W}_{t+ 1}^1, \\mathbf{W}_{t+ 1}^2, ... ,\\mathbf{W}_{t+ 1}^g]\\frac{\\partial f(\\mathbf{W}_{t+1})}{\\partial \\mathbf{W}_{t+1}} = \\mathbf{W}_{t+1} - \\mathbf{W}_{t + \\frac{1}{2}} + \\lambda [\\frac{\\mathbf{W}_{t+1}^1}{||\\mathbf{W}_{t+1}^1||_2}, \\frac{\\mathbf{W}_{t+ 1}^2}{||\\mathbf{W}_{t+ 1}^2||_2}, ... ,\\frac{\\mathbf{W}_{t+ 1}^g}{||\\mathbf{W}_{t+ 1}^g||}] = 0Finally we get: \\mathbf{W}^{t+1}_{g,i} = \\left(1 - \\frac{\\lambda}{||\\mathbf{W}_g||_2} \\right)_{+} \\mathbf{W}^{t + \\frac{1}{2}}_{g,i}Exclusive Sparsity\\mathbf{W}^{t+1}_{g,i} = \\left(1 - \\frac{\\lambda ||\\mathbf{W}_g||_1}{|w_{g,i}|} \\right)_{+} \\mathbf{W}^{t+\\frac{1}{2}}_{g,i}ExperimentsVisualization—Fully Connected Group sparsity regularizer results in the total elimination of certain features. Exclusive sparsity regularizer, when used on its own, results in disjoint feature selection for each class. When combined, it allows certain degree of feature reuse Visualization—Convolution Combined group and exclusive sparsity regularizer results in filters that are much sharper than others. Some spatial features dropped altogether from the competition with other filters.","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"Neural Network Binarization","slug":"binary-neural-network","date":"2017-10-02T13:16:04.000Z","updated":"2017-10-19T14:24:43.517Z","comments":true,"path":"2017/10/02/binary-neural-network/","link":"","permalink":"http://yoursite.com/2017/10/02/binary-neural-network/","excerpt":"","text":"BinaryConnect Binary Neural Network Binary Weight Network XNOR-Net Loss-aware Binarization of Deep Network (LAB)","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"Batch Normalization","slug":"Batch-Normalization","date":"2017-09-29T01:52:30.000Z","updated":"2017-10-19T14:23:03.473Z","comments":true,"path":"2017/09/29/Batch-Normalization/","link":"","permalink":"http://yoursite.com/2017/09/29/Batch-Normalization/","excerpt":"Internal Covariate ShiftDefinitionDistribution of each layer’s inputs between different mini-batches changes during training, as the parameters of the previous layers change:","text":"Internal Covariate ShiftDefinitionDistribution of each layer’s inputs between different mini-batches changes during training, as the parameters of the previous layers change: P\\_k(X=x) != P\\_{k+1}(X=x)where $k$ represents $k$ th mini batches. Consequence This slows down thetraining by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"About Me","slug":"About-Me","date":"2017-09-14T13:14:52.000Z","updated":"2017-09-14T13:35:22.024Z","comments":true,"path":"2017/09/14/About-Me/","link":"","permalink":"http://yoursite.com/2017/09/14/About-Me/","excerpt":"","text":"Mr. Chen ShangyuSecond year Ph.D studentNanyang Technological UniversityUnder supervision of Prof. Sinno Jialin Pan Email: csyhhu [at] gmail [dot] com Education Experience2012-2016 BS School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China2016-Now Ph.D School of Computer Science and Engineering, Nanyang Technological University, Singapore Publication Xin Dong, Shangyu Chen and Sinno Jialin Pan. Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon To appear in Annual Conference on Neural Information Processing Systems 2017 (NIPS-17). Long Beach, USA. Dec. 4-9, 2017. (Full paper)","categories":[],"tags":[]},{"title":"BirdEye - an Automatic Method for Inverse Perspective Transformation of Road Image without Calibration","slug":"IPM","date":"2015-07-09T04:32:06.000Z","updated":"2017-10-03T04:32:15.025Z","comments":true,"path":"2015/07/09/IPM/","link":"","permalink":"http://yoursite.com/2015/07/09/IPM/","excerpt":"AbstractInverse Perspective Mapping(IPM) based lane detection is widely employed in vehicle intelligence applications.Currently, most IPM method requires the camera to be calibrated in advance. Any shifts of camera will lead to the failure of the previous calibration. In this work, a calibration-free approach is proposed to iteratively attain an accurate inverse perspective transformation of the road, through which a “birdeye” view of the target road, as well as parallel lanes, can be gained. This method obtains 4 end-points of a pair of lanes in perspective image by our lane detection algorithm first. Based on the hypothesis that the road is flat, we project these points to the corresponding points in the IPM view in which the two lanes are parallel lines to get the initial transformation matrix. Then, we use an iteration strategy to increase IPM’s accuracy by minimizing the detectionerror between the lines in two views. After the iteration stops, the better transformation matrix is used for final IPM. Besides, to attain a better IPM view in which every lane approaches parallel, we propose a multi-lanes based IPM which involve more lanes. The results of several experiments are provided to demonstrate the effectiveness of the method.","text":"AbstractInverse Perspective Mapping(IPM) based lane detection is widely employed in vehicle intelligence applications.Currently, most IPM method requires the camera to be calibrated in advance. Any shifts of camera will lead to the failure of the previous calibration. In this work, a calibration-free approach is proposed to iteratively attain an accurate inverse perspective transformation of the road, through which a “birdeye” view of the target road, as well as parallel lanes, can be gained. This method obtains 4 end-points of a pair of lanes in perspective image by our lane detection algorithm first. Based on the hypothesis that the road is flat, we project these points to the corresponding points in the IPM view in which the two lanes are parallel lines to get the initial transformation matrix. Then, we use an iteration strategy to increase IPM’s accuracy by minimizing the detectionerror between the lines in two views. After the iteration stops, the better transformation matrix is used for final IPM. Besides, to attain a better IPM view in which every lane approaches parallel, we propose a multi-lanes based IPM which involve more lanes. The results of several experiments are provided to demonstrate the effectiveness of the method. Contents +1. Algorithm Overview +2. Inverse Perspective Mapping +3. Pre Process +4. Iterative Method fo IPM +5. Experiment +6. Reference Algorithm Overview&lt;/span&gt; Inverse Perspective Mapping&lt;/span&gt;In computer vision, the mathematical relationship between two planes is defined as a homography matrix H. As explainedin [1], the matrix H can be expressed as H = sMR. We can attain the transformation relationship between two planes by 8corresponding points, 4 points in each plane. The image below illustrates the process. Pre Process&lt;/span&gt;Denoising based on Lane’s characteristicThe lanes in perspective view possess some different feature from other lines, such as the lanes’ angles are alwayswithin a certain range. Therefore, we need to set a threshold to filtrate all the lines, wiping off nearly-horizontal andnearly-perpendicular lines. Moreover, the noise lines are minimal and separated in different frame of the road record.It is also useful to filter out the lines which are minority based on their angles from the total lines gathered in anumber of frames. The rest lines are the lanes we want. Bisect-Kmeans to Distinguish Different LanesReal road condition is complex, we can hardly know how many lanes in the road, which will cause great trouble to our process.Therefore, it is necessary to cluster the lines in time. In this paper we put forward a fast method especially aimed at lanedetection to get the number of the clusters. In real road image, the horizontal ordinate of points which lines cross the upperscreen possess big difference from each other. Therefore, we can perform a fast bisect-Kmeans based on crossing points: divideall the lines in two part with Kmeans, and calculate each cluster’s variance also based on the the horizontal ordinate of thecrossing points. If its variance is bigger than a certain threshold, keep Kmeans(K=2) it. Iterative Method fo IPM&lt;/span&gt;After the first-time IPM, we detect the IPM view to locate the lanes’ end points (we name them “project points”), which shouldbe the real corresponding points of perspective view’s end points. Then transform the “project points” back into the perspectiveview as “back points”. However, there maybe some inaccuracy in our IPM view’s detection, thus we can not use the back points toreplace the source points. Instead, we combine the back points with the previous source points to get new source points (Thecombining method can be various: predict-combine or weights-combine), then use these new points to start iteration. Therefore byiteration, more accurate IPM view can be attained. Experiment&lt;/span&gt; Reference&lt;/span&gt;[1] Bradski,G,”Learning OpenCV”,O’REILLY,2008","categories":[],"tags":[{"name":"Research","slug":"Research","permalink":"http://yoursite.com/tags/Research/"}]}]}