{"meta":{"title":"Chen Shangyu's Blog","subtitle":null,"description":null,"author":"Chen Shangyu","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"Weekly Paper - From Nov 19th, 2017","slug":"paper-NOV19-2017","date":"2017-11-23T12:07:14.000Z","updated":"2017-11-23T13:30:37.239Z","comments":true,"path":"2017/11/23/paper-NOV19-2017/","link":"","permalink":"http://yoursite.com/2017/11/23/paper-NOV19-2017/","excerpt":"Weekly paper reading from Nov 19th - Nov 26th, 2017","text":"Weekly paper reading from Nov 19th - Nov 26th, 2017 Neural Discrete Represesntation LearningPaper in One Sentence:跟VAE很相似，只不过每个sample encode成了一个字典（大matrxi）里面的一个词（即一个向量）。 NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune ParadigmPaper in One Sentence:自动生成weights, neuron, 以及去边 Architecture Flow in NeST：NeST begins with an initial seed architecture, typically a sparse and partially connected NN. Then, it utilizes two sequential phases to synthesize the NN: gradient-based growth phase magnitude-based pruning phase Gradient-based GrowthConnection (weights) Growth Given the initial network, NeST locate the ‘dormant’ connections that can reduce loss effectively. It evaluates $|\\frac{\\partial L }{\\partial w}|$ as the dormant score for every connection. And activates the connections with large gradient magnitudes. Learning the Number of Neurons in Deep NetworksPaper in One Sentence:加了一个group sparsity, 训练. Group sparsity regularizer可以一次去掉一个neuron. 这就是所谓的“学出neurons的数量” Group Sparsity Regularizer Proximal Gradient Desecnt","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"Deep Learning as A Mixed Convex Combinatorial Optimization Problem","slug":"Deep-Learning-as-A-Mixed-Convex-Combinatorial-Optimization-Problem","date":"2017-11-15T13:16:08.000Z","updated":"2017-11-15T13:22:29.632Z","comments":true,"path":"2017/11/15/Deep-Learning-as-A-Mixed-Convex-Combinatorial-Optimization-Problem/","link":"","permalink":"http://yoursite.com/2017/11/15/Deep-Learning-as-A-Mixed-Convex-Combinatorial-Optimization-Problem/","excerpt":"","text":"Paper in One SentenceTrain neural network with hard-threshold activations, by separating network into layer-wise and linear separable.把网络训练问题转化成类似layer-wise的每层网络训练： 找到一组能linear separate当前mini-batch的weights.","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"Domain Adaption VS Transfer Learning","slug":"DomainAdaptionTransferLearning","date":"2017-11-13T12:29:09.000Z","updated":"2017-11-19T11:28:35.619Z","comments":true,"path":"2017/11/13/DomainAdaptionTransferLearning/","link":"","permalink":"http://yoursite.com/2017/11/13/DomainAdaptionTransferLearning/","excerpt":"","text":"","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"Network Sparsity","slug":"Network-Sparsity","date":"2017-11-05T03:19:18.000Z","updated":"2017-11-19T13:30:56.484Z","comments":true,"path":"2017/11/05/Network-Sparsity/","link":"","permalink":"http://yoursite.com/2017/11/05/Network-Sparsity/","excerpt":"This article is about some methods to sparsify neural network by training. Recently I have read two paper published by Sung Ju Hwang group in ICML 2017. I think these two papers share some ideas. Put them here to be compared.","text":"This article is about some methods to sparsify neural network by training. Recently I have read two paper published by Sung Ju Hwang group in ICML 2017. I think these two papers share some ideas. Put them here to be compared. Combined Group and Exclusive Sparisty for Deep Neural NetworkPaper in one sentenceThis paper adds two regularizers: Group sparsity and exclusive sparsity, to train neural network. Motivation In the optimal case, the weights at each layer will be fully orthogonal to each other, and thus forming an orthogonal basis set. To do so, enforce network weights at each layer to fit to different sets of input features as much as possible. (exclusive sparsity) However, it is not practical nor desirable to restrict each weight to be completely disjoint from others as some features still need to be shared (think about low level features). Therefore introduce an additional group sparsity regularizer based on (2, 1)-norm. (group sparsity) Idea Visiualization Group Sparsity tries to delete input neurons. Exclusive Sparsity tries to make output neurons fight for input neurons. That is to say input neurons belong to only one output neuron. FormulationGroup Sparsity\\Omega (\\mathbf{W}) = \\sum_g ||\\mathbf{W}_g^l||_2 = \\sum_g \\sqrt{\\sum_i (w_{g,i}^l)^2}where: $g$ represents group, in this paper, every input neuron and its corresponding weights are a group. $l$: layer of neural network Obviously we can see, it does a group lasso in input neuron: do 2-norm at group first, then do 1-norm between groups. Exclusive Sparsity\\Omega (\\mathbf{W}) = \\frac{1}{2} \\sum_g ||\\mathbf{W}_g^l||_1^2 = \\frac{1}{2} \\sum_g (\\sum_i |w_{g,i}^l|)^2Some thinkingsIt is easy to knwo using group lasso can result in group sparsity. But it confuses me why using (1,2)-norm can “make output neurons compete for inputs”. As is said in paper: Applying 2-norm over these 1-norm groups will result in even weights among the groups; that is, all groups should have similar number of non-sparse weights, and thus no group can have large number of non-sparse weight. Also, exclusive sparsity was first introduced in Multi-task learning: Hierarchical Classification via Orthogonal Transfer Maybe can find more details in this paper. OptimizationIt uses Proximal Gradient Descent: First obtains the intermediate solution \\mathbf{W}_{t+\\frac{1}{2}} by taking a gradient step using the gradient computed on the loss only. Then optimize for the regularization term while performing Euclidean projection of it to the solution space:\\min_{\\mathbf{W}_{t+1}} = \\{\\Omega (\\mathbf{W}_{t+\\frac{1}{2}}) + \\frac{1}{2\\lambda} || \\mathbf{W}_{t+1} - \\mathbf{W}_{t+\\frac{1}{2} }||_2^2 \\} Group Sparsity proximal stepSet f(\\mathbf{W}_{t+1}) = \\frac{1}{2} ||\\mathbf{W}_{t+1} - \\mathbf{W}_{t + \\frac{1}{2}}||^2_2 + \\underbrace{\\lambda [\\mathbf{W}_{t+ 1}^1, \\mathbf{W}_{t+ 1}^2, ... ,\\mathbf{W}_{t+ 1}^g]}_{\\text{Regularizer}}\\frac{\\partial f(\\mathbf{W}_{t+1})}{\\partial \\mathbf{W}_{t+1}} = \\mathbf{W}_{t+1} - \\mathbf{W}_{t + \\frac{1}{2}} + \\lambda [\\frac{\\mathbf{W}_{t+1}^1}{||\\mathbf{W}_{t+1}^1||_2}, \\frac{\\mathbf{W}_{t+ 1}^2}{||\\mathbf{W}_{t+ 1}^2||_2}, ... ,\\frac{\\mathbf{W}_{t+ 1}^g}{||\\mathbf{W}_{t+ 1}^g||}] = 0Finally we get: \\mathbf{W}^{t+1}_{g,i} = \\left(1 - \\frac{\\lambda}{||\\mathbf{W}_g||_2} \\right)_{+} \\mathbf{W}^{t + \\frac{1}{2}}_{g,i}Exclusive Sparsity\\mathbf{W}^{t+1}_{g,i} = \\left(1 - \\frac{\\lambda ||\\mathbf{W}_g||_1}{|w_{g,i}|} \\right)_{+} \\mathbf{W}^{t+\\frac{1}{2}}_{g,i}ExperimentsVisualization—Fully Connected Group sparsity regularizer results in the total elimination of certain features. Exclusive sparsity regularizer, when used on its own, results in disjoint feature selection for each class. When combined, it allows certain degree of feature reuse Visualization—Convolution Combined group and exclusive sparsity regularizer results in filters that are much sharper than others. Some spatial features dropped altogether from the competition with other filters.","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"Neural Network Binarization","slug":"binary-neural-network","date":"2017-10-02T13:16:04.000Z","updated":"2017-10-19T14:24:43.517Z","comments":true,"path":"2017/10/02/binary-neural-network/","link":"","permalink":"http://yoursite.com/2017/10/02/binary-neural-network/","excerpt":"","text":"BinaryConnect Binary Neural Network Binary Weight Network XNOR-Net Loss-aware Binarization of Deep Network (LAB)","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"Batch Normalization","slug":"Batch-Normalization","date":"2017-09-29T01:52:30.000Z","updated":"2017-11-09T02:22:19.887Z","comments":true,"path":"2017/09/29/Batch-Normalization/","link":"","permalink":"http://yoursite.com/2017/09/29/Batch-Normalization/","excerpt":"Internal Covariate ShiftDefinitionDistribution of each layer’s inputs between different mini-batches changes during training, as the parameters of the previous layers change:","text":"Internal Covariate ShiftDefinitionDistribution of each layer’s inputs between different mini-batches changes during training, as the parameters of the previous layers change: P_k(X=x) != P_{k+1}(X=x)where $k$ represents $k$ th mini batches. Consequence This slows down thetraining by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.","categories":[],"tags":[{"name":"Paper Reading","slug":"Paper-Reading","permalink":"http://yoursite.com/tags/Paper-Reading/"}]},{"title":"About Me","slug":"About-Me","date":"2017-09-14T13:14:52.000Z","updated":"2017-09-14T13:35:22.024Z","comments":true,"path":"2017/09/14/About-Me/","link":"","permalink":"http://yoursite.com/2017/09/14/About-Me/","excerpt":"","text":"Mr. Chen ShangyuSecond year Ph.D studentNanyang Technological UniversityUnder supervision of Prof. Sinno Jialin Pan Email: csyhhu [at] gmail [dot] com Education Experience2012-2016 BS School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China2016-Now Ph.D School of Computer Science and Engineering, Nanyang Technological University, Singapore Publication Xin Dong, Shangyu Chen and Sinno Jialin Pan. Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon To appear in Annual Conference on Neural Information Processing Systems 2017 (NIPS-17). Long Beach, USA. Dec. 4-9, 2017. (Full paper)","categories":[],"tags":[]},{"title":"BirdEye - an Automatic Method for Inverse Perspective Transformation of Road Image without Calibration","slug":"IPM","date":"2015-07-09T04:32:06.000Z","updated":"2017-11-06T03:23:21.087Z","comments":true,"path":"2015/07/09/IPM/","link":"","permalink":"http://yoursite.com/2015/07/09/IPM/","excerpt":"AbstractInverse Perspective Mapping(IPM) based lane detection is widely employed in vehicle intelligence applications.Currently, most IPM method requires the camera to be calibrated in advance. Any shifts of camera will lead to the failure of the previous calibration. In this work, a calibration-free approach is proposed to iteratively attain an accurate inverse perspective transformation of the road, through which a “birdeye” view of the target road, as well as parallel lanes, can be gained. This method obtains 4 end-points of a pair of lanes in perspective image by our lane detection algorithm first. Based on the hypothesis that the road is flat, we project these points to the corresponding points in the IPM view in which the two lanes are parallel lines to get the initial transformation matrix. Then, we use an iteration strategy to increase IPM’s accuracy by minimizing the detectionerror between the lines in two views. After the iteration stops, the better transformation matrix is used for final IPM. Besides, to attain a better IPM view in which every lane approaches parallel, we propose a multi-lanes based IPM which involve more lanes. The results of several experiments are provided to demonstrate the effectiveness of the method.","text":"AbstractInverse Perspective Mapping(IPM) based lane detection is widely employed in vehicle intelligence applications.Currently, most IPM method requires the camera to be calibrated in advance. Any shifts of camera will lead to the failure of the previous calibration. In this work, a calibration-free approach is proposed to iteratively attain an accurate inverse perspective transformation of the road, through which a “birdeye” view of the target road, as well as parallel lanes, can be gained. This method obtains 4 end-points of a pair of lanes in perspective image by our lane detection algorithm first. Based on the hypothesis that the road is flat, we project these points to the corresponding points in the IPM view in which the two lanes are parallel lines to get the initial transformation matrix. Then, we use an iteration strategy to increase IPM’s accuracy by minimizing the detectionerror between the lines in two views. After the iteration stops, the better transformation matrix is used for final IPM. Besides, to attain a better IPM view in which every lane approaches parallel, we propose a multi-lanes based IPM which involve more lanes. The results of several experiments are provided to demonstrate the effectiveness of the method. Contents +1. Algorithm Overview +2. Inverse Perspective Mapping +3. Pre Process +4. Iterative Method fo IPM +5. Experiment +6. Reference Algorithm Overview&lt;/span&gt; Inverse Perspective Mapping&lt;/span&gt;In computer vision, the mathematical relationship between two planes is defined as a homography matrix H. As explainedin [1], the matrix H can be expressed as H = sMR. We can attain the transformation relationship between two planes by 8corresponding points, 4 points in each plane. The image below illustrates the process. Pre Process&lt;/span&gt;Denoising based on Lane’s characteristicThe lanes in perspective view possess some different feature from other lines, such as the lanes’ angles are alwayswithin a certain range. Therefore, we need to set a threshold to filtrate all the lines, wiping off nearly-horizontal andnearly-perpendicular lines. Moreover, the noise lines are minimal and separated in different frame of the road record.It is also useful to filter out the lines which are minority based on their angles from the total lines gathered in anumber of frames. The rest lines are the lanes we want. Bisect-Kmeans to Distinguish Different LanesReal road condition is complex, we can hardly know how many lanes in the road, which will cause great trouble to our process.Therefore, it is necessary to cluster the lines in time. In this paper we put forward a fast method especially aimed at lanedetection to get the number of the clusters. In real road image, the horizontal ordinate of points which lines cross the upperscreen possess big difference from each other. Therefore, we can perform a fast bisect-Kmeans based on crossing points: divideall the lines in two part with Kmeans, and calculate each cluster’s variance also based on the the horizontal ordinate of thecrossing points. If its variance is bigger than a certain threshold, keep Kmeans(K=2) it. Iterative Method fo IPM&lt;/span&gt;After the first-time IPM, we detect the IPM view to locate the lanes’ end points (we name them “project points”), which shouldbe the real corresponding points of perspective view’s end points. Then transform the “project points” back into the perspectiveview as “back points”. However, there maybe some inaccuracy in our IPM view’s detection, thus we can not use the back points toreplace the source points. Instead, we combine the back points with the previous source points to get new source points (Thecombining method can be various: predict-combine or weights-combine), then use these new points to start iteration. Therefore byiteration, more accurate IPM view can be attained. Experiment&lt;/span&gt; Reference&lt;/span&gt;[1] Bradski,G,”Learning OpenCV”,O’REILLY,2008","categories":[],"tags":[{"name":"Research","slug":"Research","permalink":"http://yoursite.com/tags/Research/"}]},{"title":"A Saliency SIFT Feature-Based Method for Image Recommendation","slug":"Saliency-SIFT-image-recommendation","date":"2015-07-08T13:16:31.000Z","updated":"2017-12-21T01:54:18.500Z","comments":true,"path":"2015/07/08/Saliency-SIFT-image-recommendation/","link":"","permalink":"http://yoursite.com/2015/07/08/Saliency-SIFT-image-recommendation/","excerpt":"AbtractCurrent image search and image recommendation show their boundedness in accuracy, because these methods tend to neglect images’ content while focus on textual information searching in the Internet. In order to fully employ images’ information, such as color, style, texture to perform recommendation which is more similar to human’s recognition, a saliency-based SIFT feature extracted method is proposed to acquire detailed information of images. What’s more, a bag-of-words model is applied to better represent images. Finally based on the feature extracted, our recommendation method takes advantage of SVM to depict images’ possibility to certain image category to calculateimage’s distance to users, which approximates human’s view in comparing images. What’s more, a method to extend imagecategories to achieve accurate classification is put forward.","text":"AbtractCurrent image search and image recommendation show their boundedness in accuracy, because these methods tend to neglect images’ content while focus on textual information searching in the Internet. In order to fully employ images’ information, such as color, style, texture to perform recommendation which is more similar to human’s recognition, a saliency-based SIFT feature extracted method is proposed to acquire detailed information of images. What’s more, a bag-of-words model is applied to better represent images. Finally based on the feature extracted, our recommendation method takes advantage of SVM to depict images’ possibility to certain image category to calculateimage’s distance to users, which approximates human’s view in comparing images. What’s more, a method to extend imagecategories to achieve accurate classification is put forward. Contents +1. Algorithm Overview +2. Saliency +3. SIFT bag-of-words +4. Recommendation System +5. Experiment +6. Reference Algorithm Overview SaliencyIn our paper, we used the method in [1] to achieve saliency detection with a robust result. Firstly, a new background measure from a conceptual perspective is derived and then an effective computation method is described. Secondly, the method further discusses the unique benefits originating from its intuitive geometrical interpretation. Finally, the method proposes a principled framework that intuitively integrates low level cues and directly aims for the goal to combine multiple saliency cues or measures. SIFT bag-of-wordsSIFT featureScale-invariant feature transform (SIFT) algorithm transforms an image into a large collection of feature vectors, each of which is invariant to image translation, scaling, and rotation,partially invariant to illumination changes and robust to localgeometric distortion. Bag-of-words ModelThe bag-of-words(BOW) model originates from natural language processing and information retrieval, commonly used in methods of document classification, where the (frequency of) occurrence of each word is used as a feature for training a classifier. In our method, SIFT features will be firstly detected. Every image is described as a M 128 matrix (M represents the number of the SIFT detected). However, different numbers of SIFT features are extracted in one image, which cause difficulty in processing. To solve the problem of dimensional unequality, all image’s SIFT features are collected and clustered into K clusters (K is the user definite) to form the codebook which is a K128 matrix, and every row is the centroid of a cluster). Codebook can be viewed as a standard of features where every image’s SIFT features find its nearest centroid in the codebook. After all the features of a image are mapped to the codebook, a K dimension histogram is used to record the frequency of every matched codeword, representing the image in the BOW model. Algorithm.1 describes the process. SVM Based SimilarityImages contains various objects and partitions. Even art master can not definitely state two images are similar, while people tend to attribute images to certain categories or how much it belongs to certain categories. Therefore, assumption can be made that images are similar because that they belongs to the same category or they partly belongs to some same categories. Therefore, we propose a similarity calculation method to measure the distance between two images by multiplying how much image i belongs to category c and how much image j belongs to category c for all the categories. Recommendation SystemTo define our recommendation algorithm, first to define the records used in our methods. As a user browses images (In our experiment, what the users browse come from training set), the system will record the image index that the user get interested in. For a given user, record will be {imagei; imagej; imagek; …}. For a given record, the algorithm first specifies the user’s reference, or how much the user favors certain category. For instance, the algorithm will calculate the categories distribution in the records, category one for 40%, category two for 30% etc. For every image in the recommended set (In our experiment,the test set), the algorithm will calculate their distance from the user by summing up image’s possibility to certain category multiply how much the user are in favor of this category. ExperimentThe figure above demonstrate 5 set of experiment results. Every set contain 5 input images and 3 output(recommended images). The left column images are users’ interested images while the right ones are recommended. The display order of recommended images is according to the how strongly the system want to recommend the image to the user. The left ones are the most recommended images. Reference[1] W. Zhu, S. Liang, Y. Wei, and J. Sun, “Saliency optimization from robust background detection,” in Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, 2014, pp. 2814 – 2821.","categories":[],"tags":[{"name":"Research","slug":"Research","permalink":"http://yoursite.com/tags/Research/"}]}]}